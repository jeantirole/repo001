LINUX 


PS
# check the program status 

PS AUX 
# check the status of program background




Virtual total 

pipe line

ps aux | grep apach 
# search for apach 

kill -9 [PID #]
# halt the program of PID # 


sudo apt install mlocate 
# Ubunto 에서는.. apt-get 

 
sudo yum install mlocate  .



## ./  Definition ##

The dot( . ) is the current directory and 

The slash( / ) is a path delimiter.


dot dot( .. ) means that the directory above the current directory 

refer to the one directory further up in the hierarchy 


Ex) find ./ -name *.log 


~ 

# ( ~ ) means "user folder" 

Ex) /home/eric 


whereis [ls] 

whereis [명령어]


# To become a super user 

su - root  

pw 요구함 

su psswd [username]

password update possible 



# When you can't log in with the root ID, you should use "sudo" command for the admin right. 


#

useradd -m min 

#

cd /home
ls -l

[username] list should be there 



# 1) log in with root and other users
su - root 
su - [username]

# 2) create two users and setup the passwords  
useradd -m eric2
useradd -m eric3

passwd [username]  
*There is no permission signal, you should log in with "root" 


# 3) log in with other user id 
su - eric 





# Daemon

Description:

Client - Internet - Server 



# apache2

sudo apt-get install apache2 

sudo service apache2 start 
ps aux | grep apache2 


sudo service apache2 stop 
ps aux | grep apache2 




# Hot Key 

alias l='ls -al'




# executin with log-in 

nano .bashrc

history 

histsize



#

cd ~ 

su - eric

nano .bashrc



# Machine Snapshot 찍을 수 있음 => 다시 돌아가기 가능.. 



############################################################

Machine Learning 


# Gradient Boosting 

오차를 줄여가면서 머신을 돌림. 

다중공선성~ 


Validation test 


train test train

train train test


cross_val_score -> 교차검증을 이용해서 점수를 계산. 

cross_val_score(model, X, y, score) 



from sklearn.model_selection import cross_val_score

from sklearn.tree import DecisionTreeRegressor

from sklearn.neighbors import KNeighborsRegressor

from sklearn.ensemble import RandomForestRegressor

from sklearn.ensemble import AdaBoostRegressor

from xgboost as xgb 


###########################################################

XGBoost 

Bagging Vs Boosting

>>Bagging 의 경우, 노드들의 결과 값을 다수결에 의한 투표로 결정하게 됨 

>>Boosting 의 경우, 예측 실패값 만을 모아서 새로 모델을 돌림. 
Y = w1x2 + w2x2 + wnxn + error 방식 

Overfitting 될 가능성이 매우 높음 


n_estimators : 결정 트리의 개수

max_depth : 트리의 깊이

colsample_bytree : 컬럼의 샘플링 비율 









